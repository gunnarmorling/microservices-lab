== Change Data Capture with Kafka Connect and Debezium
:imagesdir: ./images

This part of the lab introduces you to change data capture (CDC) using http://debezium.io/[Debezium].
Debezium allows you to capture data changes from MySQL, PostgreSQL, MongoDB, Oracle and SQL Server and stream those events into Apache Kafka.
It is based on https://kafka.apache.org/documentation/#connect[Kafka Connect].
Streaming data changes from your database enables many interesting use cases such as

* Replicating data across databases by different vendors
* Synchronizing data between microservices
* Updating fulltext search indexes (e.g. Elasticsearch) or caches (e.g. https://www.redhat.com/en/technologies/jboss-middleware/data-grid[JBoss Data Grid])
* Running stream queries such as sliding averages of the value of incoming orders etc.

In this module of the lab you'll learn the following things:

* Setting up Kafka Connect with the Debezium CDC connector for MySQL
* Configuring an instance of that connector, ingesting changes from an existing example CRUD application
* Different ways of processing change data events
** Streaming data changes to a PostgreSQL sink database
** Streaming data changes to an Elasticsearch index

The overall architecture of this lab module looks like this:

image::debezium-demo.png[Module 3 Architecture Overview,width=800,height=421]

=== Setting Up the Example Application

For this part of the lab we've prepared a small https://github.com/debezium/microservices-lab/tree/master/eventr[CRUD microservice for managing orders of events], which we'll use in the following as source of data change events.
This one is based on https://thorntail.io/[Thorntail], an approach for building microservices using Java EE and MicroProfile, and uses MySQL as its database.

Start a MySQL instance by running:

[source, sh]
$ oc new-app --name=mysql debezium/example-mysql:0.9 \
    -e MYSQL_ROOT_PASSWORD=debezium \
    -e MYSQL_USER=mysqluser \
    -e MYSQL_PASSWORD=mysqlpw

To build and start the application, we're using the source-to-image process as in the previous module:

[source,sh]
----
$ oc new-app --name=eventrapp debezium/msa-lab-s2i:latest~https://github.com/debezium/microservices-lab \
    --context-dir=eventr \
    -e AB_PROMETHEUS_OFF=true \
    -e JAVA_OPTIONS=-Djava.net.preferIPv4Stack=true
----

We still need to expose port 8080 for the application and set up a route for it
(as that's not done automatically by the S2I builder image).
To do so, use `oc patch` and expose a route for the service like so:

[source,sh]
----
$ oc patch service eventrapp -p '{ "spec" : { "ports" : [{ "name" : "8080-tcp", "port" : 8080, "protocol" : "TCP", "targetPort" : 8080 }] } } }'

service/eventrapp patched

$ oc expose svc eventrapp

route.route.openshift.io/eventrapp exposed
----

Verify that the pods of the database and the application are running:

[source,sh]
----
$ oc get pods
----

Once the build has completed, you should see the following status
(the build job must be "Completed", the actual application and the database "Running"):

[source,sh]
----
NAME                                          READY     STATUS    RESTARTS   AGE
eventrapp-1-build                             0/1       Completed   0          2m
eventrapp-1-m4rss                             1/1       Running     0          39s
mysql-2-gk8nw                                 1/1       Running     0          2m
...
----

The hostname of the exposed eventrapp service is available in the OpenShift console, or can be retrieved via CLI:

[source]
$ oc get routes eventrapp -o=jsonpath='{.spec.host}{"\n"}'

Note the output, which should be in the format eventrapp-amq-streams.apps-<YOUR GUID>.generic.opentlc.com and you can navigate there using a browser.
Alternatively, you could click on the application's URL within the OpenShift web UI.

You can use "Populate Data" to create some example data (Click on "Search orders" after that to see the created entries).
Otherwise just insert a few records as you like.

=== Setting Up Kafka Connect With the Debezium Connectors

Now it's time to deploy Kafka Connect and Debezium.
Kafka Connect is a platform for streaming data between Apache Kafka and other systems.
It provides a framework and runtime environment for _source connectors_ (for getting data into Kafka)
and _sink connectors_ (for getting data out of Kafka).
Note it is a separate process from Apache Kafka itself, which can optionally be run in a clustered way, too.
The Kafka Connect node(s) then will connect to the Apache Kafka cluster and put data into Kafka topics and get data out of them.
Debezium provides a set of CDC source connectors for databases such as MySQL, PostgreSQL, SQL Server and MongoDB.

Let's begin by setting up a Kafka Connect cluster,
using the S2I process provided by Strimzi.
This makes it very easy to create a Kafka Connect cluster with additional connectors such as the ones provided by Debezium (the operation must be executed as the `admin`):

[source]
----
$ oc apply -f examples/templates/cluster-operator/connect-s2i-template.yaml -n amq-streams

template.template.openshift.io/strimzi-connect-s2i created
----

[source]
----
$ oc process strimzi-connect-s2i \
    -p CLUSTER_NAME=debezium \
    -p KAFKA_CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR=1 \
    -p KAFKA_CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR=1 \
    -p KAFKA_CONNECT_STATUS_STORAGE_REPLICATION_FACTOR=1 \
    -p KAFKA_CONNECT_BOOTSTRAP_SERVERS=production-ready-kafka-bootstrap:9092 \
    | oc apply -f -

kafkaconnects2i.kafka.strimzi.io/debezium created
----

Use `oc status` to verify that the S2I process has finished
(a deployment of `svc/debezium-connect-api` should be in state "deployed"):

[source]
----
$ oc status

In project amq-streams on server https://master00.example.com:443

svc/debezium-connect-api - 172.30.17.34:8083
  dc/debezium-connect deploys istag/debezium-connect:latest <-
    bc/debezium-connect source builds uploaded code on istag/debezium-connect-source:1.0.0
      not built yet
    deployment #2 deployed about a minute ago - 1 pod
    deployment #1 failed 2 minutes ago: newer deployment was found running
...
----

Alternatively, you can check in the OpenShift web UI that the rolling deployment for "debezium-connect" has finished.

Then download the following files:

* the Debezium CDC connector for MySQL
* the Confluent JDBC sink connector and the PostgreSQL database driver
* the Confluent Elasticsearch sink connector and its dependencies

Extract the downloaded files and trigger another S2I build of `debezium-connect`, this time including these additional resources:

[source,sh]
----
export DEBEZIUM_VERSION=0.9.4.Final
mkdir -p plugins && cd plugins && \
curl http://central.maven.org/maven2/io/debezium/debezium-connector-mysql/$DEBEZIUM_VERSION/debezium-connector-mysql-$DEBEZIUM_VERSION-plugin.tar.gz | tar xz; \
curl http://central.maven.org/maven2/io/debezium/debezium-connector-postgres/$DEBEZIUM_VERSION/debezium-connector-postgres-$DEBEZIUM_VERSION-plugin.tar.gz | tar xz; \
mkdir confluent-jdbc-sink && cd confluent-jdbc-sink && \
curl -O http://central.maven.org/maven2/org/postgresql/postgresql/42.2.2/postgresql-42.2.2.jar && \
curl -O http://packages.confluent.io/maven/io/confluent/kafka-connect-jdbc/5.0.0/kafka-connect-jdbc-5.0.0.jar && \
cd .. && \
mkdir confluent-es-sink && cd confluent-es-sink && \
curl -sO http://packages.confluent.io/maven/io/confluent/kafka-connect-elasticsearch/5.0.0/kafka-connect-elasticsearch-5.0.0.jar && \
curl -sO http://central.maven.org/maven2/io/searchbox/jest/2.0.0/jest-2.0.0.jar && \
curl -sO http://central.maven.org/maven2/org/apache/httpcomponents/httpcore-nio/4.4.4/httpcore-nio-4.4.4.jar && \
curl -sO http://central.maven.org/maven2/org/apache/httpcomponents/httpclient/4.5.1/httpclient-4.5.1.jar && \
curl -sO http://central.maven.org/maven2/org/apache/httpcomponents/httpasyncclient/4.1.1/httpasyncclient-4.1.1.jar && \
curl -sO http://central.maven.org/maven2/org/apache/httpcomponents/httpcore/4.4.4/httpcore-4.4.4.jar && \
curl -sO http://central.maven.org/maven2/commons-logging/commons-logging/1.2/commons-logging-1.2.jar && \
curl -sO http://central.maven.org/maven2/commons-codec/commons-codec/1.9/commons-codec-1.9.jar && \
curl -sO http://central.maven.org/maven2/org/apache/httpcomponents/httpcore/4.4.4/httpcore-4.4.4.jar && \
curl -sO http://central.maven.org/maven2/io/searchbox/jest-common/2.0.0/jest-common-2.0.0.jar && \
curl -sO http://central.maven.org/maven2/com/google/code/gson/gson/2.4/gson-2.4.jar && \
cd .. && \
oc start-build debezium-connect --from-dir=. --follow && \
cd ..
----

You should see an output like this:

[source]
----
Uploading directory "." as binary input for the build ...
build "debezium-connect-2" started
Receiving source from STDIN as archive ...
Assembling plugins into custom plugin directory /tmp/kafka-plugins
Moving plugins to /tmp/kafka-plugins
Pushing image docker-registry.default.svc:5000/l1099-kafka/debezium-connect:latest ...
Pushed 6/9 layers, 67% complete
Pushed 7/9 layers, 78% complete
Pushed 8/9 layers, 89% complete
Pushed 9/9 layers, 100% complete
Push successful
----

Use `oc get pods` again to verify that Kafka Connect is running:

[source,sh]
----
$ oc get pods

NAME                                          READY     STATUS    RESTARTS   AGE
debezium-connect-3-mpscv                      1/1       Running     0          1m
...
----

Once that's the case, register an instance of the Debezium MySQL connector using the REST API of Kafka Connect.
Change to the tooling pod and submit the following request with httpie:

[source]
----
echo '{
    "connector.class": "io.debezium.connector.mysql.MySqlConnector",
    "tasks.max": "1",
    "database.hostname": "mysql",
    "database.port": "3306",
    "database.user": "debezium",
    "database.password": "dbz",
    "database.server.id": "184054",
    "database.server.name": "dbserver1",
    "database.whitelist": "inventory",
    "database.history.kafka.bootstrap.servers": "production-ready-kafka-bootstrap:9092",
    "database.history.kafka.topic": "schema-changes.inventory",
    "decimal.handling.mode" : "double",
    "transforms": "route",
    "transforms.route.type": "org.apache.kafka.connect.transforms.RegexRouter",
    "transforms.route.regex": "([^.]+)\\.([^.]+)\\.([^.]+)",
    "transforms.route.replacement": "$3"
}' | http PUT http://debezium-connect-api:8083/connectors/inventory-connector/config
----

This sets up an instance of Debezium's `io.debezium.connector.mysql.MySqlConnector` class,
using the given credentials.
By specifying the `database.whitelist` option (or, on a more fine-grained level, `table.whitelist`), we can narrow down the set of captured tables.

Kafka Connect’s log file should contain messages regarding execution of initial snapshot (look for log messages like "INFO Step 1 ..."):

[source,sh]
----
$ oc logs $(oc get pods -o name -l strimzi.io/name=debezium-connect)
----

You can examine CDC messages in Kafka using kafkacat (use Ctrl + C to exit the tool):

[source]
----
$ kafkacat -b production-ready-kafka-bootstrap \
   -t EventrOrder \
   -o beginning \
   -f 'offset: %o, key: %k, value: %s\n'
----

At this point you should see messages originating from the initial snapshot performed by the connector.

Note that by default topic names follow the pattern "<db server name>.<db name>.<table name>".
By means of the `RegexRouter` in the connector configuration we've changed that so that the topic name is just the unqualified table name.
You should see messages comprising of a key and a value like the following (formatted for the sake readability),
representing the `Order` records as per the initial snapshot.

Key:

[source]
----
{
    "schema": {
        "type": "struct",
        "fields": [
            {
                "type": "int32",
                "optional": false,
                "field": "id"
            }
        ],
        "optional": false,
        "name": "dbserver1.inventory.EventrOrder.Key"
    },
    "payload": {
        "id": 4
    }
}
----

Value:

[source]
----
{
    "schema": {
        "type": "struct",
        "fields": [
            {
                "type": "struct",
                "fields": [
                    {
                        "type": "int32",
                        "optional": false,
                        "field": "id"
                    },
                    {
                        "type": "string",
                        "optional": true,
                        "field": "customer"
                    },
                    {
                        "type": "int32",
                        "optional": false,
                        "name": "io.debezium.time.Date",
                        "version": 1,
                        "field": "order_date"
                    },
                    {
                        "type": "bytes",
                        "optional": false,
                        "name": "org.apache.kafka.connect.data.Decimal",
                        "version": 1,
                        "parameters": {
                            "scale": "2",
                            "connect.decimal.precision": "19"
                        },
                        "field": "payment"
                    },
                    {
                        "type": "int32",
                        "optional": false,
                        "field": "event_id"
                    }
                ],
                "optional": true,
                "name": "dbserver1.inventory.EventrOrder.Value",
                "field": "before"
            },
            {
                "type": "struct",
                "fields": [
                    {
                        "type": "int32",
                        "optional": false,
                        "field": "id"
                    },
                    {
                        "type": "string",
                        "optional": true,
                        "field": "customer"
                    },
                    {
                        "type": "int32",
                        "optional": false,
                        "name": "io.debezium.time.Date",
                        "version": 1,
                        "field": "order_date"
                    },
                    {
                        "type": "bytes",
                        "optional": false,
                        "name": "org.apache.kafka.connect.data.Decimal",
                        "version": 1,
                        "parameters": {
                            "scale": "2",
                            "connect.decimal.precision": "19"
                        },
                        "field": "payment"
                    },
                    {
                        "type": "int32",
                        "optional": false,
                        "field": "event_id"
                    }
                ],
                "optional": true,
                "name": "dbserver1.inventory.EventrOrder.Value",
                "field": "after"
            },
            {
                "type": "struct",
                "fields": [
                    {
                        "type": "string",
                        "optional": true,
                        "field": "version"
                    },
                    {
                        "type": "string",
                        "optional": false,
                        "field": "name"
                    },
                    {
                        "type": "int64",
                        "optional": false,
                        "field": "server_id"
                    },
                    {
                        "type": "int64",
                        "optional": false,
                        "field": "ts_sec"
                    },
                    {
                        "type": "string",
                        "optional": true,
                        "field": "gtid"
                    },
                    {
                        "type": "string",
                        "optional": false,
                        "field": "file"
                    },
                    {
                        "type": "int64",
                        "optional": false,
                        "field": "pos"
                    },
                    {
                        "type": "int32",
                        "optional": false,
                        "field": "row"
                    },
                    {
                        "type": "boolean",
                        "optional": true,
                        "default": false,
                        "field": "snapshot"
                    },
                    {
                        "type": "int64",
                        "optional": true,
                        "field": "thread"
                    },
                    {
                        "type": "string",
                        "optional": true,
                        "field": "db"
                    },
                    {
                        "type": "string",
                        "optional": true,
                        "field": "table"
                    },
                    {
                        "type": "string",
                        "optional": true,
                        "field": "query"
                    }
                ],
                "optional": false,
                "name": "io.debezium.connector.mysql.Source",
                "field": "source"
            },
            {
                "type": "string",
                "optional": false,
                "field": "op"
            },
            {
                "type": "int64",
                "optional": true,
                "field": "ts_ms"
            }
        ],
        "optional": false,
        "name": "dbserver1.inventory.EventrOrder.Envelope"
    },
    "payload": {
        "before": null,
        "after": {
            "id": 4,
            "customer": "Bob Smith",
            "order_date": 17829,
            "payment": "F28=",
            "event_id": 1
        },
        "source": {
            "version": "0.8.3.Final",
            "name": "dbserver1",
            "server_id": 223344,
            "ts_sec": 1540457930,
            "gtid": null,
            "file": "mysql-bin.000003",
            "pos": 101280,
            "row": 0,
            "snapshot": false,
            "thread": 182,
            "db": "inventory",
            "table": "EventrOrder",
            "query": null
        },
        "op": "c",
        "ts_ms": 1540457964571
    }
}
----

Message key and value use JSON (the binary Avro format could be used alternatively),
and both contain a payload as well as a schema describing the structure of the payload.

The key's payload resembles the primary key of the represented record.
The value's payload contains information of

* the old state of the changed row (`before`, which is null in the case of an insert or record created during snapshotting)
* the new state of the changed row (`after`)
* metadata such as the table and database name, a timestamp etc.

If you now use the web app to insert, update or delete records while keeping the console consumer running, you'll see how corresponding CDC messages arrive in the topic.

Using the Kafka Connect REST API, you also can query the list of connectors, query the status of a given connector, delete a connector and more
(all these command should be run in the tooling pod):

[source]
----
# List all connectors
$ http http://debezium-connect-api:8083/connectors
----

[source]
----
# Get status of "inventory-connector"
$ http http://debezium-connect-api:8083/connectors/inventory-connector/status
----

[source]
----
# Restart "inventory-connector"
$ http POST http://debezium-connect-api:8083/connectors/inventory-connector/restart
----

[source]
----
# Delete "inventory-connector" (don't run it, as we'll still need the connector in the following)
$ http DELETE http://debezium-connect-api:8083/connectors/inventory-connector
----

=== Consuming Change Data Events

Examining change events in the Kafka console is a good first step,
but eventually we'd like to consume the events in a more meaningful way.

In the following different ways for consuming events are explored.
You can choose the one you are most interested in or walk through all the alternatives,
as your preference.

==== Streaming Data Changes to a PostgreSQL Sink Database

To stream data changes into another database, no manual programming effort is needed.
Instead, the Confluent JDBC sink connector for Kafka Connect can be used to stream data into a target database.

So let's set up another database (PostgreSQL in this case) and stream the data changes there.

[source]
----
$ oc new-app \
    -e POSTGRESQL_USER=postgresuser \
    -e POSTGRESQL_PASSWORD=postgrespw \
    -e POSTGRESQL_DATABASE=inventory \
    centos/postgresql-95-centos7
----

Once the database has started (use `oc get pods` to verify that PostgreSQL is running), register an instance of the https://docs.confluent.io/current/connect/kafka-connect-jdbc/sink-connector/index.html[Kafka Connect JDBC sink connector].
This connector can be used to propagate Kafka messages to relational databases via JDBC.
Change to the tooling pod and submit the following request with httpie:

[source]
----
echo '{
    "connector.class": "io.confluent.connect.jdbc.JdbcSinkConnector",
    "tasks.max": "1",
    "topics": "EventrOrder",
    "connection.url": "jdbc:postgresql://postgresql-95-centos7:5432/inventory?user=postgresuser&password=postgrespw",
    "transforms": "unwrap",
    "transforms.unwrap.type": "io.debezium.transforms.UnwrapFromEnvelope",
    "auto.create": "true",
    "insert.mode": "upsert",
    "pk.fields": "id",
    "pk.mode": "record_value"
}' | http PUT http://debezium-connect-api:8083/connectors/jdbc-sink/config
----

This sets up an an instance of `io.confluent.connect.jdbc.JdbcSinkConnector`,
listening to the `EventrOrder` topic and streaming all data changes to the given database connection.
As this sink connector just expects the effective state of changed rows
(i.e. the "after" part from the Debezium data change messages),
only this part is extracted using Debezium's `UnwrapFromEnvelope` SMT (single message transform).

With the sink connector being set up, we can take a look into the PostgreSQL database and see how the table changes are propgated there.
Still in the tooling pod, execute:

[source,sh]
----
$ pgcli postgresql://postgresuser:postgrespw@postgresql-95-centos7:5432/inventory
----

Run a query to get all records from the table corresponding to the monitored topic:

[source]
----
select * from "EventrOrder";
----

As you alter records in the source web application,
you'll see how the table in PostgreSQL gets updated accordingly, if you re-execute the query.
Note that `DELETE` operations currently cannot be propagated, as they are not yet supported by the Kafka Connect JDBC sink connector.
Debezium provides a solution for that by allowing deletes to be rewritten into updates of a logical "deleted" field in emitted messages.
This can then be used to delete all records in the sink database e.g. using a batch job.

To leave the shell on the PostgreSQL pod, run:

[source]
----
exit
----

==== Streaming Change Events To Elasticsearch

The beauty of using Apache Kafka for streaming change events is its flexibility.
As the topics are persistent, additional consumers can come up which have not been known when data changes originally occurred.

As an example, lets stream the `EventrOrder` events to Elasticsearch now, too, making them available to the powerful fulltext search capabilities.

Set up a single Elasticsearch node
(it'd be a complete cluster in production, but a single node is fine for the purposes of this lab)
and expose it as a service:

[source]
----
$ oc new-app -e ES_JAVA_OPTS="-Xms512m -Xmx512m" elasticsearch:6.4.2
----

[source]
----
$ oc expose svc/elasticsearch

route.route.openshift.io/elasticsearch exposed
----

Create a configuration file for Elasticsearch:

[source]
----
$ cat > elasticsearch.yml << EOF
cluster.name: my-es-cluster
network.host: 0.0.0.0
discovery.zen.minimum_master_nodes: 1
discovery.type: single-node
EOF
----

And make its contents available as a config map:

[source]
----
$ oc create configmap es-config --from-file=elasticsearch.yml

configmap/es-config created
----

Finally, the config map contents can be exposed to the Elasticsearch container using a volume:

[source]
----
$ oc set volumes dc/elasticsearch --overwrite --add \
  -t configmap \
  -m /usr/share/elasticsearch/config/elasticsearch.yml \
  --sub-path=elasticsearch.yml \
  --name=es-config \
  --configmap-name=es-config

warning: volume "es-config" did not previously exist and was not overriden. A new volume with this name has been created instead.deploymentconfig.apps.openshift.io/elasticsearch volume updated
----

This triggers a restart of the Elasticsearch node; once it's up again (use `oc get pods` to verify),
it's time to register an instance of the Elasticsearch sink connector (change to the tooling pod for this):

[source]
----
echo '{
    "connector.class": "io.confluent.connect.elasticsearch.ElasticsearchSinkConnector",
    "tasks.max": "1",
    "topics": "EventrOrder",
    "connection.url": "http://elasticsearch:9200",
    "key.ignore": "false",
    "type.name": "order",
    "behavior.on.null.values" : "delete",
    "topic.index.map" : "EventrOrder:eventrorder",
    "transforms": "unwrap,key",
    "transforms.unwrap.type": "io.debezium.transforms.UnwrapFromEnvelope",
    "transforms.key.type": "org.apache.kafka.connect.transforms.ExtractField$Key",
    "transforms.key.field": "id"
}' | http PUT http://debezium-connect-api:8083/connectors/elastic-sink/config
----

This listens to the `EventrOrder` topic and pushes corresponding index updates to Elasticsearch.
As index names must be lower-cased, the topic is named to the "eventrorder" index name.
The `UnwrapFromEnvelope` transformation is used to extract only the "after" state from Debezium's change events.
Using the `ExtractField` transformation we make sure that the original record id is used as the document id in Elasticsearch.

If the connector is deployed, you can query the index via its REST API.

Get its URL by executing:

[source]
$ oc get routes elasticsearch -o=jsonpath='{.spec.host}{"\n"}'

Open that URL in a browser, it should be in the form http://elasticsearch-amq-streams.apps-<YOUR GUID>.generic.opentlc.com.

To browse the contents of the `eventrorder` index, go to http://elasticsearch-amq-streams.apps-<YOUR GUID>.generic.opentlc.com/eventrorder/_search?pretty.
Alternatively, you can use httpie in the tooling pod to do so:

[source]
$ http elasticsearch:9200/eventrorder/_search?pretty

Again create or update a few orders in the event application and you'll see how the Elasticsearch index is updated based on that.

==== Clean-Up

To wrap up this section and make room for more explorations,
remove all the applications created during this part of the lab:

[source]
$ oc delete all -l app=eventrapp
$ oc delete all -l app=elasticsearch
$ oc delete all -l app=postgresql-95-centos7
$ oc delete all -l app=mysql

So far, we've used ready-made Kafka Connect sink connectors for consuming change events and propagating them to sinks like another database or a search index.
Let's explore in the next chapter some advanced topics: how can change events be consumed by other microservices in a programmatic way, and how can they be subject to streaming queries with the Apache Kafka Streams API?
Find out in <<module-04#,module 4>>!
